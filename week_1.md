# 1주차 수업: 기계학습 개요, 퍼셉트론

## 목차

1. 기계학습 개요
2. 퍼셉트론

## 기계학습 개요

기계학습(Machine Learning, ML)이란 컴퓨터가 학습을 할 수 있도록 만드는 방법론을 말한다. 이때 학습이란, 주어진 데이터로부터 유의미한 정보를 표현하는 모델을 인간의 도움 없이 만드는 것이다.

많이 인용되는 Tom M Mitchell의 정의에 따르면, "컴퓨터가 **어떤 작업 T를 경험 E를 통해 평가 P를** **향상**시킬 때, 컴퓨터가 학습한다"고 할 수 있다.

[^출처]: https://en.wikipedia.org/wiki/Machine_learning

기계학습의 세부 패러다임은 크게 3가지가 있다.

* 지도 학습(Supervised Learning): 입력(문제)과 출력(답)을 모두 알려주는 방식
* 강화 학습(Reinforcement Learning): 입력은 알려주지만, 출력에 대한 긍정/부정적 피드백만 존재하고, 무엇이 올바른 출력인지는 알려주지 않는 방식
* 비지도 학습(Unsupervised Learning): 입력만 알려주는 방식

### 혼동하기 쉬운 용어들

#### 인공지능(Artificial Intelligence, AI)

인공지능은 컴퓨터가 주체가 되는 모든 지능을 말하며 매우 피상적인 개념이다. 깊게 파고 들어가면 지능이 무엇인가라는 의문점에 도달하게 된다. 여기서는 가볍게 분야로서의 인공지능을 다룬다.

인공지능은 "기성적인 관점에서 컴퓨터는 못하고 사람은 할 수 있는 일"을 컴퓨터가 해내게 만드는 것을 주 목표로 삼는다. 인공지능이 다루는 주제의 예시로서 패턴 인식(ex: 음성 인식, 지문 인식 등), 자연어 처리(ex: 기계 번역, 챗봇 등), 추론 등이 있다.

기계학습은 인공지능을 획득하기 위한 하나의 방법론에 불과하다.

#### 빅데이터(Big Data)

빅데이터는 양과 차원이 압도적으로 높은 데이터 혹은 그런 데이터를 다루는 분야다. 특히 이런 데이터로부터 유의미한 관계를 찾는 것을 데이터 마이닝(Data Mining)이라 부른다.

기계학습은 데이터 마이닝의 수단으로서 사용된다.

#### 인공신경망(Artificial Neural Network)과 딥러닝(Deep Learning)

인공신경망은 다수의 퍼셉트론을 수평/수직으로 쌓아서 만든 모델로, 기계학습을 실현하는 도구 중 하나다. 기계학습이 꼭 인공신경망만 있는 건 아니다. SVM, HMM, 유전 알고리즘, 결정트리 등 수많은 다른 모델과 방법론이 존재한다.

딥러닝은 제대로 정의된 용어가 아닌데, 인공신경망을 깊게 쌓았더니 잘 되길래 붙여진 애칭 정도로 보는 것이 옳다.

인공신경망을 지탱하는 매우 중요한 도구가 '미분'이다. 미분은 인공신경망이 학습을 할 수 있는 원동력이다.

### 기계학습에서 다루는 문제들

#### 분류(Classification)



분류 문제는 어떤 입력이 어떤 카테고리에 속하는지 결정하는 문제다. 이를 조금 더 수학적으로 이야기하면, 임의의 입력을 셀 수 있는 집합(Countable Set)에 1:1 대응을 시키는 것이다.

yes/no를 결정하는 문제도 분류 문제에 속한다.

ex: 어떤 캐릭터 이미지를 줬을 때, 이 캐릭터가 내 최애인지 아닌지 판단하기, 비속어 탐지기

#### 회귀(Regression)

회귀 문제는 둘 혹은 그 이상의 데이터 사이의 관계를 찾는 문제다. 쉽게 말하면 함수를 찾아내는 것이다. 전통적으로 통계학에서 회귀 분석을 다뤄왔는데, 기계학습이 유행하면서 이쪽으로도 많이 넘어왔다.

ex: 어떤 지역의 주거지의 위치, 평수 등의 정보를 주고 월세를 예측하기, 그래프 커브 피팅

전통적인 관점의 회귀에는 포함되지 않지만, 확률분포를 찾거나 요즘 유행하는 도메인 간 변환시키는 것도 차원이 높은 회귀의 일종으로 볼 수 있다.

ex: MakeGirlMoe, style2paints

#### 클러스터 분석(Clustering)



클러스터 분석은 분류 문제와 유사하지만, 그 클래스 자체를 찾아내는 문제다. 유사한 데이터들을 그렇지 않은 데이터와 구분하고 응집시킨 뒤, 그 중심점을 찾는다.

ex: 색상 클러스터링, 영상/컨텐츠 추천 시스템

#### 특징 추출(Feature Learning)

어떤 데이터 속에 숨어있는 유의미한 저차원 정보를 특징(Feature)이라고 부른다. 사람의 얼굴은 아주 다양하지만 분명 공통점이 존재한다. (눈의 평균적인 위치 등) 이런 것들을 뽑아내고 활용하는 문제를 기계학습에서 많이 다룬다.

ex: 얼굴인식, illustration2vec

### 기계학습과 관련된 학문

* 인공지능
* 빅데이터
* 병렬연산
* 통계
* 다변수 해석학
* 최적화 이론

------

## 퍼셉트론(Perceptron)

퍼셉트론의 정의나 구현 등은 책을 참고하도록 한다. 여기에는 중요한 이슈만 요약하였다.

1. 단층 퍼셉트론으로 NOT, OR, AND, NAND를 구현할 수 있다.
2. 단층 퍼셉트론으로 XOR은 절대로 구현할 수 없다.
3. 2층 퍼셉트론으로 XOR을 구현할 수 있다.
4. 2층 퍼셉트론으로 모든 임의의 논리회로를 구현할 수 있다.

**~ TMI**

책에서 소개한 퍼셉트론은 {0, 1}의 값을 출력하도록 돼 있는데, 퍼셉트론을 처음 고안한 학자(Frank Rosenblatt)는 {-1, 1}을 출력하도록 했다. 하지만 값의 범위가 변경되었다 하더라도 위 사실은 변하지 않는다.

#### 선형 구분 가능(Linearly Separable)

어떤 유클리드 공간에 존재하는 두 데이터 집단을 1개의 초평면(Hyperplane)만으로 구분할 수 있을 때, 이 집단들을 선형 구분 가능이라고 한다. 단층 퍼셉트론으로 구현할 수 있는 아이들은 모두 선형 구분 가능하다.

[^설명]: 초평면이 받아들이기 너무 어렵다면, 2차원 공간에서 직선을 그어 구분할 수 있냐고 생각하면 된다.

#### 단층 퍼셉트론의 학습

책에서는 혼선을 우려해 퍼셉트론의 가중치 학습 방식을 적어두지 않은 모양이다. 그러나 단층 퍼셉트론은 델타 룰(Delta Rule)이라는 알고리즘을 쓰면 학습을 시킬 수 있다.

델타 룰은 경사하강법(Gradient Descent)을 단층 퍼셉트론에 적용한 것이다. 왜 이게 되는지는 경사하강법 시간에 다루도록 하고, 여기서는 학습 방식만 적어놓는다.

알파(α)는 0.001 ~ 0.01을 사용하자. 너무 크면 난리난다.

```pseudocode
f(x) = 1 if x ≥ θ
          0 else.

// 가중치와 바이어스를 초기화한다.
w[:] := random number from (-∞, ∞)
b := 0

// 오차가 충분히 줄어들 때까지 학습한다.
err := ∞
while err < threshold
	err := 0
	foreach x in X
		// 학습데이터 x에 대한 퍼셉트론의 출력 y를 구한다
		// d는 학습 목표 값이다.
		s := b
		foreach i in 0 ... N-1
			s := s + x[i] * w[i]
		y := f(s)
		
		// 오차를 더해준다.
		err := err + |y - d|
		
		// 목표값 d와 출력 y 사이의 오차를 전달한다.
		foreach i in 0 ... N-1		
			w[i] := w[i] - α * (y - d) * x[i]
		b := b - α * (y - d)
```



#### 다층 퍼셉트론의 학습

불가능하다. 뒤에서 다시 다루겠지만, 퍼셉트론은 미분이 안되기 때문에 각 가중치에 대한 교정값을 계산할 수가 없다.

------

## 숙제

1. 입력변수가 4개인 AND 게이트를 단층 퍼셉트론으로 학습시킨 뒤, 가중치의 값을 출력해보자. 어떨 땐 무한루프 돌 수도 있는데, 지극히 정상이므로 재시도를 하면 된다.
2. (x, y)를 넣었을 때 x만 뱉는 논리회로가 존재한다고 가정하자. 단층 퍼셉트론으로 이것을 구현할 수 있을까? 그 이유는?