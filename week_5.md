# 5주차: 최적화 기법

## 목차

1. 서론
2. 최적화 기법
3. 오차역전파법 구현 준비
    1. ReLU
    2. Sigmoid
    3. Affine
    4. Softmax-with-Loss
4. 오차역전파법 구현하기

## 서론

4주차 학습 때 오차역전파(Error Backward-Propagation, BP) 알고리즘을 알아보았다. BP를 수행하여 얻는 것은 각 파라미터들의 (손실함수에 대한) 그라디언트였다. 여태까지는 확률적 경사하강법(Stochastic Gradient Descent, SGD)을 사용하여 파라미터에 그라디언트를 빼면 최적화가 된다 정도로만 알고 있었다. 오늘은 SGD를 포함한 다양한 최적화 기법을 소개하고, 이들을 다루는 방법을 알아보자.

## 최적화 기법(Optimization Method)

**최적화 기법(Optimization Method)**이란, 어떤 함수의 극대값(Maxima)과 극소값(Minima)을 찾는 방법을 말한다. 아예 이것만 연구하는 최적화 이론(Optimization Theory)이라는 분야가 있을 정도로 깊고 넓은 분야다.

[^]: 알고리즘 경시대회를 준비했던 사람이라면 담금질 기법(Simulated Annealing)이란 단어를 한 번쯤은 들어봤을 것이다. 이 알고리즘은 전산학에서 쓰이는 대표적인 최적화 기법 중 하나다.

 하지만 어떤 함수든 만능으로 빠르게 극값을 찾아주는 마술은 존재하지 않는다. 최적화 기법은 그 접근법에 따라서 엄청나게 다양하다. 그 중 기계학습에서 관심을 갖는 기법으로 **통계학적 방식(Statistical Method. ex: MCMC, EM, Bayesian)**과 **서브그라디언트 방식(Subgradient Method)** 정도가 있다.

[^]: 이외에도 무니무니 교수 덕분에 유명해진 유전 알고리즘(Genetic Algorithm)도 강화학습에서는 단골 주제로 나온다.

서브그라디언트

[^]: 서브그라디언트(Subgradient)는 기울기의 개념을 '연속이지만 부분적으로 미분 불가능한 함수'(ex: |x|, ReLU)까지 확장한 것이다. '미분 가능한 함수들'(ex: Sigmoid, tanh)의 서브그라디언트는 우리가 아는 기울기나 그라디언트와 동일하다. 엄밀한 정의는 매우 난해하므로, 그냥 기울기인가보다 하고 넘어가면 된다.

 방식은 미분을 활용하여 최적화를 한다. BP 특성상 인공신경망에서 활발히 연구하고 사용한다. 서브그라디언트 방식은 크게 **1st-order**와 **2nd-order**이 있으며, 1계도함수를 쓰냐 2계도함수를 쓰냐로 나뉘어진다. 그런데 2nd-order는 계산 시간이 오래걸리고 구현도 난해해서 이론적인 연구만 이루어지고 실전에선 쓰지 않는다. 여기서 소개하는 것은 전부 1st-order 방식에 속한다.

아래의 기법들의 의사코드 및 실제구현은 ./example/week_5_optimization.py에서 확인할 수 있다.

### Stochastic Gradient Descent

가장 단순하면서 동시에 <u>아직까지도 사용되는</u> 방식이다. 그라디언트에 학습률(Learning Rate, 혹은 Step Size라고도 함)을 곱하여 크기를 줄인 뒤, 파라미터에서 빼기만 하면 된다.

[^이름에 관해서]: 초창기에는 Gradient Descent, Mini-Batched Gradient Descent, Stochastic Gradient Descent를 전체 데이터 대상으로 계산하느냐, 미니배치를 쓰느냐, 1개식 계산하느냐로 구분했었다. 요샌 그냥 뭉뚱그려서 SGD라고 퉁친다.

##### SGD의 장점

1. 간단하다 = 계산 시간이 단축된다
2. Optimizer가 기억해야 할 변수가 없다
3. 후술할 Adaptive 기법에 비해 최종적으로 얻는 성능이 더 뛰어난 경우가 있다. (참고: [SGD > Adam??](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/))

##### SGD의 단점

![](.\img\week_5\sgd_explicit.png)

SGD의 첫 번째 단점은 <u>그라디언트가 영벡터가 되는 순간 멈춘다</u>는 점이다. 이론적으로는 볼록한(Convex) 함수에만 써야되지만, 현실은 이상과 다르다. 그라디언트가 영벡터가 될 때는 총 3가지로, 지역최적해(Local Optima)이나 안장점(Saddle Point), 기울기가 0인 변곡점(Inflection Point)에 진입하는 경우다. 그리고 안타깝게도 앞의 두 가지 상황은 딥러닝을 하다보면 자주 마주하게 된다.

SGD의 두 번째 단점은 <u>비등방성 구간을 만났을 때 심한 진동을 겪는다</u>는 점이다. 손실함수의 진동 참고.

SGD의 세 번째 단점은 <u>학습률의 유동성이 없다</u>는 점이다. 너무 크게 잡으면 진동해버리거나 발산하고, 너무 작게 잡으면 느려터진다. 그런데 손실함수라는게 기울기가 구간마다 다를 수 밖에 없다. 때문에 다른 기법에 비해 시간이 오래 걸린다. 이를 해결하기 위해서 처음에는 학습률을 크게 주고, 시간이 지날수록 감소시켜주는 방식을 사용하곤 한다.

##### 손실함수의 진동

![](https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png)

손실함수 값이 요동치는(Fluctuation) 이유는 수도없이 많지만, 가장 큰 원인은 3가지 정도가 있다. 하나는 <u>손실함수가 비등방성(Anisotropy)</u>을 갖는 경우로 쏘가리 책 194p에 잘 설명돼 있다.

두 번째는 <u>배치 데이터의 구성이 바뀌기 때문</u>이다. 엄밀히 말해 순간적인 손실함수는 배치 데이터에 따라서 변한다. 3월 모의평가 보고 열나게 공부했는데 6월 모의평가 때 출제 경향이 바뀌면 점수가 순간적으로 떨어진 경험이 있을 것이다. 이전 배치로 최적화한 결과가 다음 배치 때 무조건 좋은 결과를 낸다는 보장은 없다. 배치 크기(Batch Size)가 작으면 작을수록 극심한 변화가 나타날 수 있다.

![](.\img\week_5\bad_learning_rate.png)

세 번째는 현재 구간의 모양에 비해 <u>학습률이 너무 높은 경우</u>에 나타난다. 학습률이 영 좋지않으면 아주 긴 시간동안 진동하거나 영원히 순환한다. 심한 경우 미친듯이 진동하면서 inf로 발산하다가 NaN 크리를 맞게 된다.

위 그래프는 10 * sinc(x)에서 초기값을 2.0으로 주면 재현할 수 있다.

### Momentum

물리학에서 관성(Momentum)의 개념을 최적화에 적용한 것이다. SGD가 항상 일정한 학습률을 가지고 있어서 효율이 떨어지는 것을 보완하기 위해 제안되었다. Momentum Optimizer는 내부에 그라디언트 변화량을 저장해서 다음 루프 때 일정 비율을 더해준다. 관성의 정도를 정해주는 하이퍼파라미터 α (단 0 ≤ α < 1)가 있다. SGD는 α = 0인 특수한 경우로 볼 수 있다.

##### Momentum의 장점

1. 일반적인 상황에서는 SGD보다 수렴이 빠르다.
2. 비등방성 구조로 인한 진동에 좀 더 강하다.
3. 꽤 단순하다.
4. 얕은 지역최적해(Local Optima)를 탈출할 수 있다.

##### Momentum의 단점

1. α가 너무 크면 지나치게 빙 돌기도 하고, 재수없으면 전역최적해를 탈출할 수도 있다.
2. 전역최적해 근처에선 오히려 SGD보다 심하게 진동한다.

### Adagrad

학습을 진행하면서 학습률을 낮춰주는 것에 착안하여 제안된 방식. 내부에 그라디언트의 크기의 제곱을 누적시켜서, 학습률을 점점 낮춘다. 때문에 SGD에 비해 상대적으로 높은 초기 학습률을 준다.

그러나 Adagrad는 무조건 학습률을 낮춰버리기 때문에 시간이 지나면 학습을 멈추게 되고, 최종적으로는 SGD나 Momentum보다도 못한 성능을 내기도 했다. 얼마 뒤 더 나은 방법이 제안되면서 실용적으로는 사용하지 않는다. (하지만 딥러닝 역사에 이름을 남겼고 Optimizer 논문에서 끊임없이 언급된다)

### RMSProp

Adagrad가 계속 학습률을 낮추는 것을 막기 위해, 그라디언트 크기 제곱을 '누적'시키는 것이 아니라 지수평균을 낸다. 때문에 그라디언트 크기가 충분히 작아지면 다시 학습률이 올라간다. 단순한 방법에 비해 엄청난 학습 효율 개선 효과가 나타나서 한때 인기를 끌었다.

하이퍼파라미터 ρ (단 0 ≤ ρ < 1)는 지수평균을 내는 민감한 정도를 지정한다. 추천 값은 0.9.

특이사항으로 이 방법의 제안자는 해당 기법을 논문으로 내지 않았다. (아이디어가 도출되기까지의 과정을 토론토 대학 [강의자료에서](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) 읽을 수 있다) Adagrad에 숟가락만 얹은거라 독창성이 없다고 생각했던걸까. 진실은 저너머에.

### Adadelta

간단하게 요약하면 RMSProp에 Momentum을 섞은 것이다. 제안자는 Adagrad나 Momentum에서 그라디언트가 갱신되는 수치의 '이론적인 단위'가 잘못되었다고 지적하며, Adadelta는 그걸 고친 것이라고 주장한다. 또한 이론적인 증명을 통해 학습률 η를 설정할 필요가 없다는 것도 밝혀냈다.

[^]: 하지만 η를 붙여서 구현하는 경우도 있다. 어디까지나 이론적인 주장이기 때문이고, 이론과 실전의 괴리가 상당한 딥러닝 특성상 직접 조정하겠다는 사람도 있지 않을까.

하이퍼파라미터 ρ (단 0 ≤ ρ < 1)는 지수평균을 내는 민감한 정도를 지정한다. 추천 값은 0.9.

RMSProp에 Momentum이 붙은 형태이기 때문에 훨씬 빠를 거 같지만, 막상 써보면 항상 그런 것도 아닌 것 같다.

### Adam

2010년도 후반 가장 인기있는 기법이다. 겉으로 보면 Adadelta와 상당히 비슷한 구조를 가지고 있지만, 수학적으로 디테일한 면에서 많은 차이를 보인다. 학습 속도가 매우 빨라 모델 프로토타이핑을 할 때 많이 사용하며 몇몇 유명한 아키텍쳐에서 채택한 방식이기도 하다. 많은 사용자들이 Adam을 디폴트로 놓고 실험을 수행하는 편이다.

자그마치 하이퍼파라미터가 3개나 있다. 하나는 학습률 α, 하나는 Momentum에 관여하는 지수평균 민감도 β1, 다른 하나는 그라디언트 크기 제곱의 지수평균 민감도 β2이다. 학습률을 너무 높이면 자주 터지므로 조심해서 다뤄야 한다. 추천 값은 0.001, 0.9, 0.999.

저자는 Adam의 변종인 AdaMax

[^]: 뜨문뜨문(Sparse)한 데이터(ex: 단어, 문장 등)에 적합하다고 주장한다.

도 제안했는데 아주 사소한 차이 뿐이라 여기서는 생략하였다. 관심있으면 Adam 논문을 읽으면 된다.

### COCOB

COntinuous COin Betting의 약자로, 한동안 Optimizer의 주류였던 Adaptive 방식이 아닌 새로운 접근법을 사용한 방식이다. Adam과 비슷하거나 능가할 정도로 빠른 학습 속도를 자랑하며, 부분적으로 미분 불가능한 함수에서도 안정적으로 수렴한다. 이 기법의 최대 장점은 하이퍼파라미터가 없다는 점이다.

[^]: 엄밀히 따지면 적당한 숫자 α > 0가 있긴 한데, 막장 값으로 설정하지만 않으면 학습에 큰 차이가 없다. 저자는 100을 추천했다.

구체적인 원리는 매우 복잡한 해석학을 기반으로 하지만, 간단하게 요약하면 '승산있는 도박'을 하여 올바른 그라디언트를 예측하는 방식을 사용한다. 저자는 도박으로 비유를 하지만, 실제 계산 과정에 stochastic한 요소는 거의 없다.

써보면 괜찮기는 한데, 아직까지 큰 인기를 끌지는 못하고 있다. 위에 있는 방식들에 비해서 구현이 매우 복잡한데다, Early Stop을 안해주면 갑자기 불안정해지면서 손실함수가 발광한다. 또한 필자의 경험에 따르면 Optimizer 간의 미세조정이 요구되는 Vanilla GAN에 사용하기는 매우 부적합하였다.

논문에서 가정한 이상적인 조건과 실전에서 오는 차이 때문에, 필자는 논문에는 언급되지 않은 안전장치 ε을 추가하여 안정성을 확보하였다.

## 어떤 Optimizer를 사용해야 하는가?